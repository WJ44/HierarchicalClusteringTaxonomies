{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ontologyFile = open('./semanticOntology.json')\n",
    "ontology = json.load(ontologyFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_path = 'semanticOntology/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arrest',\n",
       " 'Assault',\n",
       " 'Fighting',\n",
       " 'Shooting',\n",
       " 'Robbery',\n",
       " 'Shoplifting',\n",
       " 'Burglary',\n",
       " 'Stealing',\n",
       " 'Explosion',\n",
       " 'Arson',\n",
       " 'Vandalism',\n",
       " 'RoadAccidents']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_classes(node):\n",
    "    classes = []\n",
    "    mapping = {}\n",
    "    labels = []\n",
    "    if 'leaf' in node.keys():\n",
    "        classes.append(node['leaf'])\n",
    "    for child in node['children']:\n",
    "        labels.append(child['name'])\n",
    "        child_classes = get_classes(child)\n",
    "        classes.extend(child_classes)\n",
    "        for c in child_classes:\n",
    "            mapping[c] = child['name']\n",
    "    node['classes'] = classes\n",
    "    node['mapping'] = mapping\n",
    "    node['labels'] = labels\n",
    "    return classes\n",
    "\n",
    "get_classes(ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:34<00:00,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './data/Train'\n",
    "test_data_dir = './data/Val'\n",
    "\n",
    "folds = 5\n",
    "test_fold = 0\n",
    "if os.path.exists(train_data_dir):\n",
    "    shutil.rmtree(train_data_dir)\n",
    "if os.path.exists(test_data_dir):\n",
    "    shutil.rmtree(test_data_dir)\n",
    "\n",
    "for category in tqdm(get_classes(ontology)):\n",
    "    os.makedirs(train_data_dir + '/' + category + '/')\n",
    "    for fold in [x for x in range(folds) if x != test_fold]:\n",
    "        files = os.listdir('./data/' + str(fold) + '/' + category + '/')\n",
    "        files = [x for x in files if x != 'desktop.ini']\n",
    "        for filename in files:\n",
    "            shutil.copy('./data/' + str(fold) + '/' + category + '/' + filename, train_data_dir + '/' + category + '/')\n",
    "\n",
    "    os.makedirs(test_data_dir + '/' + category + '/')\n",
    "    files = os.listdir('./data/' + str(test_fold) + '/' + category + '/')\n",
    "    files = [x for x in files if x != 'desktop.ini']\n",
    "    for filename in files:\n",
    "        shutil.copy('./data/' + str(test_fold) + '/' + category + '/' + filename, test_data_dir + '/' + category + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Found 7250 files belonging to 12 classes.\n",
      "Preprocessing\n"
     ]
    }
   ],
   "source": [
    "img_height, img_width = 240, 320\n",
    "batch_size = 64\n",
    "\n",
    "def preprocess(images, labels):\n",
    "    return tf.keras.applications.resnet50.preprocess_input(images), labels\n",
    "\n",
    "print('Loading dataset')\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_dir,\n",
    "    seed=0,\n",
    "    label_mode='categorical',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=None)\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "print('Preprocessing')\n",
    "train_dataset = train_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T15:41:25.180704500Z",
     "start_time": "2023-07-19T15:27:21.613588400Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset\n",
      "Epoch 1/3\n",
      "114/114 [==============================] - 34s 262ms/step - loss: 2.8208 - accuracy: 0.6321\n",
      "Epoch 2/3\n",
      "114/114 [==============================] - 29s 250ms/step - loss: 0.4825 - accuracy: 0.8142\n",
      "Epoch 3/3\n",
      "114/114 [==============================] - 29s 253ms/step - loss: 0.2751 - accuracy: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/dataset\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/dataset\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training attack\n",
      "Epoch 1/3\n",
      "31/31 [==============================] - 12s 351ms/step - loss: 5.3432 - accuracy: 0.5291\n",
      "Epoch 2/3\n",
      "31/31 [==============================] - 12s 366ms/step - loss: 0.5725 - accuracy: 0.7806\n",
      "Epoch 3/3\n",
      "31/31 [==============================] - 12s 385ms/step - loss: 0.3314 - accuracy: 0.8918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/attack\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/attack\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training non_weaponized\n",
      "Epoch 1/3\n",
      "19/19 [==============================] - 10s 433ms/step - loss: 5.8113 - accuracy: 0.6630\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 8s 393ms/step - loss: 0.3329 - accuracy: 0.8655\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 8s 389ms/step - loss: 0.1735 - accuracy: 0.9529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/non_weaponized\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/non_weaponized\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training theft\n",
      "Epoch 1/3\n",
      "48/48 [==============================] - 16s 309ms/step - loss: 5.4676 - accuracy: 0.6432\n",
      "Epoch 2/3\n",
      "48/48 [==============================] - 15s 305ms/step - loss: 0.4364 - accuracy: 0.7924\n",
      "Epoch 3/3\n",
      "48/48 [==============================] - 15s 300ms/step - loss: 0.3041 - accuracy: 0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/theft\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/theft\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training non_violent\n",
      "Epoch 1/3\n",
      "30/30 [==============================] - 13s 370ms/step - loss: 4.5294 - accuracy: 0.6948\n",
      "Epoch 2/3\n",
      "30/30 [==============================] - 11s 355ms/step - loss: 0.1815 - accuracy: 0.9408\n",
      "Epoch 3/3\n",
      "30/30 [==============================] - 11s 346ms/step - loss: 0.0551 - accuracy: 0.9901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/non_violent\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/non_violent\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training property_damage\n",
      "Epoch 1/3\n",
      "36/36 [==============================] - 14s 341ms/step - loss: 4.0129 - accuracy: 0.5956\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 12s 340ms/step - loss: 0.4139 - accuracy: 0.8364\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 12s 335ms/step - loss: 0.1784 - accuracy: 0.9443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/property_damage\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/property_damage\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fire\n",
      "Epoch 1/3\n",
      "13/13 [==============================] - 9s 554ms/step - loss: 9.5232 - accuracy: 0.5720\n",
      "Epoch 2/3\n",
      "13/13 [==============================] - 7s 483ms/step - loss: 0.4308 - accuracy: 0.8183\n",
      "Epoch 3/3\n",
      "13/13 [==============================] - 7s 488ms/step - loss: 0.2067 - accuracy: 0.9171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/fire\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/semanticOntology/fire\\assets\n"
     ]
    }
   ],
   "source": [
    "def train_nodes(node):\n",
    "    print(f'Training {node[\"name\"]}')\n",
    "    n_classes = len(node['children'])\n",
    "\n",
    "    def filter_classes(image, label):\n",
    "        return tf.py_function(func=lambda y: y in node['classes'], inp=[tf.gather(class_names, tf.math.argmax(label))], Tout=tf.bool)\n",
    "\n",
    "    def relabel(image, label):\n",
    "        new_label = tf.py_function(func=lambda y: node['labels'].index(node['mapping'][y.numpy().decode()]), inp=[tf.gather(class_names, tf.math.argmax(label))], Tout=tf.int64)\n",
    "        new_label = tf.one_hot(new_label, depth=n_classes)\n",
    "        new_label.set_shape([n_classes])\n",
    "        return image, new_label\n",
    "\n",
    "\n",
    "    train_ds = train_dataset.filter(filter_classes)\n",
    "    train_ds = train_ds.map(relabel)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "    model = Sequential()\n",
    "    resnet = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        input_shape=(img_height, img_width, 3),\n",
    "        pooling='avg',\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    resnet.trainable = False\n",
    "    model.add(resnet)\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_ds, epochs=3)\n",
    "\n",
    "    model.save(model_path + node[\"name\"])\n",
    "\n",
    "    for c in node['children']:\n",
    "        if 'leaf' not in c.keys():\n",
    "            train_nodes(c)\n",
    "\n",
    "train_nodes(ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T15:41:51.049949800Z",
     "start_time": "2023-07-19T15:41:25.182708300Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 773.38it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = './data/Val'\n",
    "img_height, img_width = 240, 320\n",
    "\n",
    "categories = get_classes(ontology)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "for category in tqdm(categories):\n",
    "    images = os.listdir(test_data_dir + '/' + category)\n",
    "    images = [test_data_dir + '/' + category + '/' + x for x in images if x != 'desktop.ini']\n",
    "    df = pd.DataFrame({'image': images})\n",
    "    df['recording'] = df['image'].map(lambda x: re.search(r'.*/([a-zA-X]+\\d+)_.*\\.jpg', x).group(1))\n",
    "    df['class'] = category\n",
    "    test_df = pd.concat([test_df, df], ignore_index=True)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
    "    image = image.numpy()\n",
    "    return image\n",
    "\n",
    "test_df['image'] = [preprocess_image(x) for x in test_df['image']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T15:44:46.969001Z",
     "start_time": "2023-07-19T15:41:51.059955800Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset\n",
      "30/30 [==============================] - 9s 203ms/step\n",
      "Testing attack\n",
      "30/30 [==============================] - 6s 198ms/step\n",
      "Testing non_weaponized\n",
      "30/30 [==============================] - 6s 200ms/step\n",
      "Testing theft\n",
      "30/30 [==============================] - 6s 201ms/step\n",
      "Testing non_violent\n",
      "30/30 [==============================] - 6s 200ms/step\n",
      "Testing property_damage\n",
      "30/30 [==============================] - 6s 200ms/step\n",
      "Testing fire\n",
      "30/30 [==============================] - 6s 199ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_dict = {}\n",
    "for category in categories:\n",
    "    prediction_dict[category] = 1\n",
    "test_df['prediction'] = [prediction_dict.copy() for _ in range(len(test_df))]\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(lambda: test_df['image'], output_signature=(tf.TensorSpec(shape=(240, 320, 3), dtype=tf.float32)))\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "\n",
    "def test_nodes(node):\n",
    "    print(f'Testing {node[\"name\"]}')\n",
    "\n",
    "    model = tf.keras.models.load_model(\"./models/\" + str(test_fold) + \"/\" + model_path + node['name'])\n",
    "    prediction = model.predict(test_ds)\n",
    "\n",
    "    def prediction_mapping(prediction):\n",
    "        classification = {}\n",
    "        for category, label in node['mapping'].items():\n",
    "            classification[category] = prediction[node['labels'].index(label)]\n",
    "        return classification\n",
    "\n",
    "    predictions = [prediction_mapping(x) for x in prediction]\n",
    "\n",
    "    def update_predictions(prediction):\n",
    "        for category in node['classes']:\n",
    "            prediction['prediction'][category] *= predictions[prediction.name][category]\n",
    "\n",
    "    test_df.apply(update_predictions, axis=1)\n",
    "    for c in node['children']:\n",
    "        if 'leaf' not in c.keys():\n",
    "            test_nodes(c)\n",
    "\n",
    "test_nodes(ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T15:44:47.020999100Z",
     "start_time": "2023-07-19T15:44:46.974999700Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class         Assault\n",
      "prediction    Assault\n",
      "Name: Abuse003, dtype: object\n",
      "['attack', 'non_weaponized', 'assault'] ['attack', 'non_weaponized', 'assault']\n"
     ]
    }
   ],
   "source": [
    "def sum_predictions(predictions):\n",
    "    final_prediction = {}\n",
    "    for category in categories:\n",
    "        final_prediction[category] = 0\n",
    "        for prediction in predictions:\n",
    "            final_prediction[category] += prediction[category]\n",
    "    return final_prediction\n",
    "\n",
    "predictions = test_df.groupby('recording').agg({'class': 'first', 'prediction': sum_predictions})\n",
    "predictions['prediction'] = predictions['prediction'].map(lambda x: max(x, key=x.get))\n",
    "\n",
    "def find_ancesotrs(prediction, ontology, pred, true):\n",
    "    for c in ontology[\"children\"]:\n",
    "        if prediction[\"prediction\"] in c[\"classes\"]:\n",
    "            pred.append(c[\"name\"])\n",
    "        if prediction[\"class\"] in c[\"classes\"]:\n",
    "            true.append(c[\"name\"])\n",
    "        find_ancesotrs(prediction, c, pred, true)\n",
    "\n",
    "def add_ancestors(prediction):\n",
    "    print(prediction)\n",
    "    pred = []\n",
    "    true = []\n",
    "    find_ancesotrs(prediction, ontology, pred, true)\n",
    "    prediction[\"pred_ancestors\"] = pred\n",
    "    prediction[\"true_ancestors\"] = true\n",
    "\n",
    "add_ancestors(predictions.iloc[0])\n",
    "# print(classification_report(predictions['class'], predictions['prediction']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
